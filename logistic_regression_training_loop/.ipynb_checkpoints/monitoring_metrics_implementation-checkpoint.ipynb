{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5e62a5-0de2-4627-9770-34a5010b9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_monitoring_metrics(system_type, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the appropriate monitoring metrics for the given system type.\n",
    "\n",
    "    Parameters:\n",
    "        system_type (str): \"classification\" or \"regression\"\n",
    "        y_true (array-like): Ground truth labels/values\n",
    "        y_pred (array-like): Predicted labels/values\n",
    "\n",
    "    Returns:\n",
    "        dict: Computed monitoring metrics\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    system_type = system_type.lower().strip()\n",
    "\n",
    "    if system_type == \"classification\":\n",
    "        # Basic classification metrics\n",
    "        accuracy = np.mean(y_true == y_pred)\n",
    "\n",
    "        # Confusion matrix parts (binary classification assumption)\n",
    "        # If your classification is multi-class, tell me and Iâ€™ll adjust it.\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0.0\n",
    "\n",
    "        metrics[\"accuracy\"] = float(accuracy)\n",
    "        metrics[\"precision\"] = float(precision)\n",
    "        metrics[\"recall\"] = float(recall)\n",
    "        metrics[\"f1_score\"] = float(f1)\n",
    "        metrics[\"tp\"] = int(tp)\n",
    "        metrics[\"tn\"] = int(tn)\n",
    "        metrics[\"fp\"] = int(fp)\n",
    "        metrics[\"fn\"] = int(fn)\n",
    "\n",
    "    elif system_type == \"regression\":\n",
    "        # Basic regression metrics\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        mse = np.mean((y_true - y_pred) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        # R^2\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0\n",
    "\n",
    "        metrics[\"mae\"] = float(mae)\n",
    "        metrics[\"mse\"] = float(mse)\n",
    "        metrics[\"rmse\"] = float(rmse)\n",
    "        metrics[\"r2_score\"] = float(r2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"system_type must be either 'classification' or 'regression'\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926fedd-8341-407c-820e-16a7d83de59e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
